import pdb
import random
import math
import copy

import pandas as pd
import numpy as np
import torch

from torch.utils.data import Dataset


class oneDataSet(Dataset):
    def __init__(self):
        self.x = torch.tensor([[-0.4545, -0.3382,  0.5685, -0.6863, -0.6523,  0.0227, -0.2503, -0.3209,
         -0.3960, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6171,  0.7012, -0.6033, -0.5126, -0.1852, -0.4033, -0.2067,
         -0.6697, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6578,  0.6321, -0.7952, -0.6351, -0.0824, -0.1054, -0.1733,
         -0.7133, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6930,  0.7493, -0.7630, -0.7008, -0.3206, -0.3258, -0.6463,
         -0.2850, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6973,  0.6145, -0.7550, -0.8116, -0.3238, -0.0819, -0.5358,
         -0.4066, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5829,  0.6020, -0.8212, -0.8588, -0.1818,  0.0596, -0.4972,
         -0.5056, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5963,  0.6147, -0.8518, -0.6514, -0.1551, -0.4113, -0.3679,
         -0.3650, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6277,  0.4940, -0.7142, -0.7122,  0.1208,  0.0615, -0.3059,
         -0.3420, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5792,  0.4142, -0.5927, -0.5835,  0.3918, -0.0549, -0.4006,
         -0.5185, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5243,  0.4638, -0.6789, -0.4329,  0.2427, -0.3700, -0.3672,
         -0.4794, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7818,  0.2988, -0.6450, -0.7888, -0.2402, -0.4587, -0.5918,
         -0.5275, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6562,  0.5898, -0.7177, -0.7616, -0.3989, -0.4074, -0.6455,
         -0.5577, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5463,  0.7126, -0.7474, -0.6887, -0.4757, -0.3794, -0.5940,
         -0.5950, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5441,  0.6020, -0.7569, -0.7142, -0.3061, -0.0991, -0.5249,
         -0.2643, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6524,  0.6461, -0.6321, -0.6437, -0.0080,  0.1938, -0.6339,
         -0.3547, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6754,  0.6228, -0.6784, -0.7447,  0.0388, -0.3523, -0.6301,
         -0.3121, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6524,  0.6142, -0.6778, -0.6591, -0.2520, -0.4883, -0.3960,
         -0.4744, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6643,  0.7175, -0.7689, -0.6109, -0.4616, -0.2413, -0.3230,
         -0.5147, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5756,  0.5578, -0.8610, -0.7503, -0.0687, -0.4111, -0.4412,
         -0.1849, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5480,  0.4725, -0.7825, -0.6564, -0.4170, -0.3867, -0.4659,
         -0.2525, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5570,  0.7473, -0.6433, -0.3225, -0.4747, -0.3335, -0.5201,
         -0.0336, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6100,  0.6882, -0.6731,  0.0250, -0.2586, -0.2084, -0.3550,
         -0.5941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.3916,  0.6477, -0.6849,  0.0400, -0.1960, -0.3792, -0.2883,
         -0.4493, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4542,  0.5808, -0.5955, -0.2171, -0.1191, -0.3724, -0.3935,
         -0.0247, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5254,  0.5406, -0.6388, -0.6163, -0.1780, -0.0860, -0.6671,
         -0.3027, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6788,  0.6622, -0.5626, -0.6125, -0.1253, -0.4629, -0.5310,
         -0.5065, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7285,  0.7164, -0.7645, -0.5616,  0.1019, -0.5410, -0.4606,
         -0.4741, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6301,  0.6354, -0.7126, -0.5748, -0.0973, -0.4235, -0.5330,
         -0.4996, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5767,  0.5491, -0.6997, -0.7001, -0.2388, -0.1525, -0.3113,
         -0.5781, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6148,  0.2157, -0.7321, -0.7286, -0.1849,  0.1590, -0.2402,
         -0.3062, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5204, -0.1491, -0.6761, -0.6008, -0.1911, -0.2153, -0.0819,
          0.0060, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4550, -0.0370, -0.8133,  0.0072, -0.1338, -0.4449, -0.2633,
          0.1366, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6451,  0.1150, -0.7239,  0.2348, -0.4244,  0.0228, -0.1802,
         -0.1671, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6721,  0.3472, -0.6940,  0.1325, -0.0625,  0.0777, -0.1188,
         -0.3388, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7261,  0.4522, -0.6935, -0.1932, -0.0628, -0.2236, -0.3585,
          0.3139, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.8265,  0.5711, -0.6046, -0.6008,  0.1112, -0.4933, -0.5965,
         -0.4807, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.8509,  0.5463, -0.6563, -0.6617,  0.1254, -0.2778, -0.6674,
         -0.5901, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4806,  0.5614, -0.8104, -0.6195, -0.1642, -0.3126, -0.5657,
         -0.5022, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4844,  0.6594, -0.7687, -0.7410,  0.0809, -0.3208, -0.6441,
         -0.2658, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5967,  0.5944, -0.7637, -0.7382, -0.2791, -0.0668, -0.6494,
         -0.2600, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5442,  0.5467, -0.7518, -0.6122, -0.5718,  0.1278, -0.6321,
         -0.4253, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.3881,  0.5425, -0.7014, -0.7754, -0.4756, -0.1751, -0.3199,
         -0.4662, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5564,  0.5317, -0.7502, -0.6440, -0.2843, -0.4520, -0.6183,
         -0.4328, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7389,  0.5521, -0.7462, -0.5512, -0.1400, -0.3010, -0.3633,
         -0.5727, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5563,  0.4825, -0.7893, -0.6633, -0.3745, -0.3112, -0.5726,
         -0.5801, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5157,  0.6204, -0.6947, -0.8026, -0.0692, -0.2751, -0.3948,
         -0.6518, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7801,  0.4700, -0.6560, -0.8050, -0.1325, -0.2226, -0.2907,
         -0.5297, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7935,  0.3372, -0.7763, -0.6446, -0.3693, -0.2418, -0.4567,
         -0.4816, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6241,  0.4209, -0.6575, -0.6695, -0.1709, -0.2372, -0.1504,
         -0.3451, -1.0000, -1.0000, -1.0000, -1.0000,  0.0000,  1.0000, -0.9000],
        [-0.4545, -0.5985,  0.6049, -0.7467, -0.6914,  0.0291, -0.1841, -0.2917,
         -0.1741, -1.0000, -1.0000, -1.0000, -1.0000,  0.0000,  1.0000, -0.9000],
        [-0.4545, -0.6717,  0.7007, -0.7821, -0.5626, -0.0010, -0.4253, -0.3019,
         -0.5104, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7190,  0.5429, -0.7737, -0.6798, -0.1339, -0.6038, -0.4894,
         -0.6712, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7181,  0.5969, -0.7690, -0.5887, -0.1313, -0.5946, -0.4758,
         -0.6189, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6980,  0.6087, -0.7025, -0.5730, -0.0862, -0.0456, -0.5721,
         -0.5153, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4767,  0.6355, -0.6616, -0.7228, -0.2803, -0.2557, -0.6505,
         -0.4721, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.3651,  0.5585, -0.7235, -0.6233, -0.4180, -0.2766, -0.4747,
         -0.5420, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.1155,  0.4456, -0.7585, -0.6493, -0.3216, -0.1468, -0.4781,
         -0.5472, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545,  0.0039,  0.5660, -0.7733, -0.6461, -0.1187, -0.0468, -0.2651,
         -0.3827, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545,  0.0944,  0.5125, -0.8398, -0.5436, -0.2861, -0.1219, -0.3608,
         -0.5038, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.2683,  0.4817, -0.6499, -0.5523, -0.1673, -0.2929, -0.4388,
         -0.7547, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6324,  0.5669, -0.6159, -0.6986, -0.2166, -0.1911, -0.4528,
         -0.3859, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5484,  0.6281, -0.8192, -0.3634, -0.1430, -0.2395, -0.5459,
         -0.1613, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6517,  0.3989, -0.8044, -0.7400, -0.3988, -0.5640, -0.5682,
         -0.1959, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6951,  0.4929, -0.7241, -0.9521, -0.0996, -0.4971, -0.4675,
         -0.4097, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5848,  0.6780, -0.6551, -0.8010, -0.2708, -0.2319, -0.5293,
         -0.2921, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6205,  0.6526, -0.6690, -0.7614, -0.2888, -0.2141, -0.4578,
         -0.3780, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6483,  0.6519, -0.7557, -0.7394, -0.3383, -0.4934, -0.3591,
         -0.4821, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6841,  0.6174, -0.7654, -0.6679, -0.6496, -0.3151, -0.4443,
         -0.4259, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5822,  0.5366, -0.7737, -0.5183, -0.6265, -0.3409, -0.6830,
         -0.4127, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4145,  0.6962, -0.7027, -0.5989, -0.6008, -0.3064, -0.6843,
         -0.2050, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4579,  0.7058, -0.7066, -0.6106, -0.4954, -0.4836, -0.6992,
         -0.1906, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6015,  0.6463, -0.6690, -0.5753, -0.0597, -0.6909, -0.3918,
         -0.5383, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6242,  0.5959, -0.6781, -0.6937,  0.1101, -0.3287, -0.5591,
         -0.4574, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.8026,  0.6363, -0.6742, -0.5533,  0.1130, -0.4441, -0.7052,
         -0.4412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6644,  0.5825, -0.7424, -0.5655, -0.0712, -0.4914, -0.6843,
         -0.6509, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5968,  0.7208, -0.6979, -0.6956, -0.3551, -0.4655, -0.2408,
         -0.4544, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6629,  0.6129, -0.8078, -0.7143, -0.5236, -0.2106, -0.3611,
         -0.5523, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5252,  0.5378, -0.7519, -0.6163, -0.4643, -0.3247, -0.4890,
         -0.3566, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.3531,  0.5552, -0.7739, -0.6648, -0.3932, -0.5309, -0.4688,
         -0.3978, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4975,  0.7224, -0.7972, -0.6604, -0.1345, -0.5718, -0.2201,
         -0.5251, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6687,  0.6305, -0.7013, -0.5988, -0.2620, -0.3994, -0.2253,
         -0.2375, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.8660,  0.6372, -0.7929, -0.1246, -0.1236, -0.3000, -0.6419,
          0.1407,  0.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6398,  0.6003, -0.7181,  0.2022,  0.2009, -0.1262, -0.5297,
         -0.6191, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5580,  0.6561, -0.7327,  0.1748, -0.4360, -0.4313, -0.2361,
         -0.5268, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6536,  0.7555, -0.6517, -0.3421, -0.6822, -0.4095, -0.1438,
          0.2783, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6466,  0.6155, -0.7109, -0.6771, -0.2488, -0.6084, -0.2472,
         -0.2623, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5888,  0.6212, -0.7265, -0.5202, -0.2748, -0.6398, -0.3355,
         -0.5041, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5668,  0.6974, -0.6706, -0.5099, -0.8638, -0.2833, -0.5397,
         -0.3141, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5570,  0.7078, -0.6272, -0.6253, -0.6426, -0.2777, -0.3273,
         -0.4111, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5221,  0.8566, -0.7173, -0.6862, -0.4712, -0.0652, -0.3455,
         -0.3373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5656,  0.8292, -0.6392, -0.8220, -0.1328, -0.2290, -0.4647,
         -0.3276, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5126,  0.6526, -0.6320, -0.6273, -0.2979, -0.3473, -0.0667,
         -0.4437, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6523,  0.5357, -0.1058, -0.7135, -0.5815, -0.4059,  0.3136,
         -0.5537, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5383,  0.4555,  0.1956, -0.6949, -0.1131, -0.0423, -0.0989,
         -0.3796, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6112,  0.6633,  0.1947, -0.6773, -0.0780,  0.2147, -0.4692,
         -0.7426, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6709,  0.5968, -0.3355, -0.7384, -0.2649, -0.1207,  0.3484,
         -0.6559, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5464,  0.4022, -0.7975, -0.6866, -0.1452, -0.2080, -0.4776,
         -0.7446, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6447,  0.4823, -0.7583, -0.6985, -0.1852, -0.1848, -0.4264,
         -0.5451, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5083,  0.6786, -0.7513, -0.6288, -0.1533, -0.4088, -0.4315,
         -0.5479, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4240,  0.6552, -0.7096, -0.7092, -0.3106, -0.4067, -0.7649,
         -0.4120, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6289,  0.5094, -0.7690, -0.6652, -0.5307, -0.0103, -0.6185,
         -0.2498, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6032,  0.4860, -0.6379, -0.5779, -0.0581,  0.1741, -0.4851,
         -0.5109, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6742,  0.4754, -0.6010, -0.6287,  0.1471, -0.1168, -0.3058,
         -0.5365, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6506,  0.4602, -0.6798, -0.6865,  0.1395, -0.1291, -0.5816,
         -0.4965, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7305,  0.6244, -0.5582, -0.7066,  0.0174, -0.4827, -0.3284,
         -0.7027, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6855,  0.7033, -0.6664, -0.7070, -0.2828, -0.6278, -0.4060,
         -0.6489, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5585,  0.5623, -0.7417, -0.6429, -0.5625, -0.2167, -0.4223,
         -0.2403, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5412,  0.4517, -0.7618, -0.6250, -0.1507,  0.1074, -0.2988,
         -0.4648, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4931,  0.3788, -0.7698, -0.7539, -0.5098, -0.0143, -0.1915,
         -0.6098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5642,  0.4710, -0.7392, -0.6578, -0.4440, -0.4920, -0.3312,
         -0.2815, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5955,  0.6075, -0.7310, -0.6418, -0.5884,  0.0141, -0.1937,
         -0.2724, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6146,  0.6899, -0.6710, -0.7826, -0.5667, -0.2555, -0.2622,
         -0.4761, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6571,  0.6027, -0.7595, -0.7784, -0.4201, -0.5311, -0.2797,
         -0.6492, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6503,  0.5156, -0.7451, -0.6604, -0.4632, -0.1129, -0.3801,
         -0.3658, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7269,  0.4682, -0.5763, -0.6298, -0.2575, -0.3112, -0.6713,
         -0.3074, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6353,  0.5917, -0.6833, -0.7249, -0.0528,  0.0423, -0.5357,
         -0.3187, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4325,  0.5190, -0.6526, -0.6486, -0.2185, -0.4711, -0.4694,
         -0.2752, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6503,  0.5901, -0.5715, -0.5256,  0.1930, -0.1377, -0.7175,
         -0.3401, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5497,  0.6101, -0.6883, -0.6067,  0.2534, -0.3262, -0.4185,
         -0.4262, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.1795,  0.6285, -0.7497, -0.6864,  0.1707, -0.2177, -0.1193,
         -0.6313, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.0193,  0.5228, -0.7255, -0.6557, -0.1484, -0.1230, -0.1520,
         -0.3095, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545,  0.0686,  0.4682, -0.7786, -0.5457, -0.5630, -0.1272, -0.5036,
         -0.3897, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.0927,  0.5200, -0.6177, -0.5412, -0.5042, -0.3377, -0.4842,
         -0.5707, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4681,  0.4167, -0.6667, -0.6101, -0.1016, -0.4015, -0.3533,
         -0.1583, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.4592,  0.5302, -0.8212, -0.7122, -0.4309, -0.0109, -0.4512,
         -0.3006, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6373,  0.6111, -0.7395, -0.7705, -0.3110, -0.0058, -0.4700,
         -0.4653, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.7161,  0.5984, -0.7122, -0.7425, -0.1915, -0.0999, -0.6304,
         -0.4375, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.5764,  0.5011, -0.7112, -0.8045, -0.0713, -0.0295, -0.4763,
         -0.5570, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000],
        [-0.4545, -0.6134,  0.5960, -0.7019, -0.8213, -0.1654,  0.0262, -0.4376,
         -0.7027, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.9000]])
        self.y = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1.])

    def __len__(self):
        return 1

    def __getitem__(self, index):
        return self.x, self.x.shape[0], self.y


class MicrosoftTrainDataset(Dataset):
    def __init__(self, all_seqs, min_feature, max_feature):
        self.all_seqs = copy.deepcopy(all_seqs)
        self.processed_data, self.processed_label = self.preprocess(
            self.all_seqs, min_feature, max_feature
        )
        # pdb.set_trace()

    def preprocess(self, all_seqs, min_feature, max_feature):
        # for each sequence, correct all possible training data and labels
        # Note: max look back is 128 steps, encoder is unrolled 64 steps
        look_back_steps = 128
        unrolled_steps = 64
        features = []
        labels = []
        for one_seq in all_seqs:
            length = len(one_seq)
            for i in range(0, length - 1):
                lower_bound = max(0, i - look_back_steps)
                upper_bound = min(length, i + 1 + unrolled_steps)

                one_seq_array = np.array(one_seq)
                x = one_seq_array[lower_bound : i + 1][:, :-1]
                y = one_seq_array[i + 1 : upper_bound][:, -1]

                # perform min-max transformation
                x = 2 * (x - min_feature) / (max_feature - min_feature) - 1

                x = torch.from_numpy(x).float()
                y = torch.from_numpy(y).float()
                features.append(x)
                labels.append(y)

        return features, labels

    def __len__(self):
        return len(self.processed_data)

    def __getitem__(self, idx):
        x = self.processed_data[idx]
        length = len(x)
        y = self.processed_label[idx]
        # pdb.set_trace()
        return x, length, y


class MicrosoftTestDataset(Dataset):
    def __init__(self, all_seqs):
        self.all_seqs = copy.deepcopy(all_seqs)

    def __len__(self):
        return len(self.all_seqs)

    def __getitem__(self, idx):
        return self.all_seqs[idx]


def process_data(csv_file, split=[0.5, 0.2, 0.3]):
    raw_data = pd.read_csv(csv_file)

    data_copy = raw_data.copy()
    data_copy["model"] = data_copy["model"].apply(lambda x: float(x[5:]))
    data_copy["failure"] = data_copy["failure"].apply(lambda x: float(x))
    data_copy.drop(columns=["datetime"], inplace=True)
    # pdb.set_trace()
    data_array = data_copy.to_numpy(dtype=float)  # (58300, 17)

    max_feature = np.max(data_array, axis=0)[:-1]
    min_feature = np.min(data_array, axis=0)[:-1]

    data_array = data_array.reshape((100, 583, 17))

    # find each sequence that leads to failure
    all_seqs = []
    for i in range(0, 100):
        one_seq = []
        for j in range(0, 583):
            assert data_array[i][j][0] == i + 1  # check machine id

            if data_array[i][j][-1] == 0.0:
                # no failure, add to sequence
                one_seq.append(data_array[i][j])
            elif data_array[i][j][-1] == 1.0:
                # failure occurs, add if the sequence is no empty
                if len(one_seq) > 0:
                    one_seq.append(data_array[i][j])
                    all_seqs.append(one_seq)
                    one_seq = []
            else:
                assert False

    random.shuffle(all_seqs)
    sizes = np.array(split) * len(all_seqs)

    train_sz, valid_sz = math.floor(sizes[0]), math.floor(sizes[1])
    train_dataset = MicrosoftTrainDataset(
        all_seqs[0:train_sz], min_feature, max_feature
    )
    valid_dataset = MicrosoftTrainDataset(
        all_seqs[train_sz : train_sz + valid_sz], min_feature, max_feature
    )
    test_dataset = MicrosoftTestDataset(all_seqs[train_sz + valid_sz :])

    return train_dataset, valid_dataset, test_dataset


if __name__ == "__main__":
    process_data("../AMLWorkshop/Data/features_15h.csv")
    # MicrosoftTrainDataset("../AMLWorkshop/Data/features_15h.csv")
    # MicrosoftDataset("/Users/yuningwang/Desktop/CS535_final_project/AMLWorkshop/Data/features_15h.csv")
